{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Learning\n",
    "\n",
    "Let's describe the \"taxi problem\". We want to build a self-driving taxi that can pick up passengers at one of a set of fixed locations, drop them off at another location, and get there in the quickest amount of time while avoiding obstacles.\n",
    "\n",
    "The AI Gym lets us create this environment quickly: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : |\u001b[43m \u001b[0m: |\n",
      "|\u001b[35mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import gym # importa a biblioteca do gym\n",
    "import random  \n",
    "\n",
    "random.seed(1234)\n",
    "\n",
    "streets = gym.make(\"Taxi-v3\", render_mode='ansi').env # cria o ambiente do taxi, que consiste em um grid 5x5 com um taxi, um passageiro e um destino\n",
    "# e o objetivo é levar o passageiro até o destino no menor caminho possível através do algoritmo de reinforcement learning\n",
    "streets.reset()\n",
    "print(\"\\n\"+streets.render())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nesse caso o passageiro é indicado pela cor Azul, o destino pela cor Roxa e o taxi pela cor Amarela"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's break down what we're seeing here:\n",
    "\n",
    "-  R, G, B, and Y are pickup or dropoff locations.\n",
    "-  The BLUE letter indicates where we need to pick someone up from.\n",
    "-  The MAGENTA letter indicates where that passenger wants to go to.\n",
    "-  The solid lines represent walls that the taxi cannot cross.\n",
    "-  The filled rectangle represents the taxi itself - it's yellow when empty, and green when carrying a passenger."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our little world here, which we've called \"streets\", is a 5x5 grid. The state of this world at any time can be defined by:\n",
    "\n",
    "-  Where the taxi is (one of 5x5 = 25 locations)\n",
    "-  What the current destination is (4 possibilities)\n",
    "-  Where the passenger is (5 possibilities: at one of the destinations, or inside the taxi)\n",
    "\n",
    "So there are a total of 25 x 4 x 5 = 500 possible states that describe our world.\n",
    "\n",
    "For each state, there are six possible actions:\n",
    "\n",
    "-  Move South, East, North, or West\n",
    "-  Pickup a passenger\n",
    "-  Drop off a passenger\n",
    "\n",
    "Q-Learning will take place using the following rewards and penalties at each state:\n",
    "\n",
    "-  A successfull drop-off yields +20 points\n",
    "-  Every time step taken while driving a passenger yields a -1 point penalty\n",
    "-  Picking up or dropping off at an illegal location yields a -10 point penalty\n",
    "\n",
    "Moving across a wall just isn't allowed at all.\n",
    "\n",
    "Let's define an initial state, with the taxi at location (2, 3), the passenger at pickup location 2, and the destination at location 0:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'+---------+\\n|R: | : :G|\\n| : | : : |\\n| : : : : |\\n| | : | : |\\n|\\x1b[34;1mY\\x1b[0m| :\\x1b[43m \\x1b[0m|\\x1b[35mB\\x1b[0m: |\\n+---------+\\n\\n'"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "initial_state = streets.encode(2, 3, 2, 0) # define o estado inicial do taxi, onde o taxi está na posição 2,3, o passageiro na posição 2, e o destino na posição 0\n",
    "\n",
    "streets.s = initial_state # define o estado inicial\n",
    "\n",
    "streets.render() # renderiza o ambiente\n",
    "# print(\"\\n\"+streets.render()) # imprime o ambiente com melhor visualização, no entanto nao mostra as localizações corretas\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's examine the reward table for this initial state:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: [(1.0, 368, -1, False)],\n",
       " 1: [(1.0, 168, -1, False)],\n",
       " 2: [(1.0, 288, -1, False)],\n",
       " 3: [(1.0, 248, -1, False)],\n",
       " 4: [(1.0, 268, -10, False)],\n",
       " 5: [(1.0, 268, -10, False)]}"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "streets.P[initial_state]\n",
    "# o dicionário abaixo mostra as possíveis ações que o taxi pode tomar em cada estado, e o que acontece quando ele toma cada ação\n",
    "# o 0 representa a ação de mover para o sul, o 1 para o norte, o 2 para o leste, o 3 para o oeste, \n",
    "# o 4 para pegar o passageiro e o 5 para deixar o passageiro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's how to interpret this - each row corresponds to a potential action at this state: move South, North, East, or West, pickup, or dropoff. The four values in each row are the probability assigned to that action, the next state that results from that action, the reward for that action, and whether that action indicates a successful dropoff took place. \n",
    "\n",
    "So for example, moving North from this state would put us into state number 368, incur a penalty of -1 for taking up time, and does not result in a successful dropoff.\n",
    "\n",
    "So, let's do Q-learning! First we need to train our model. At a high level, we'll train over 10,000 simulated taxi runs. For each run, we'll step through time, with a 10% chance at each step of making a random, exploratory step instead of using the learned Q values to guide our actions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "q_table = np.zeros([streets.observation_space.n, streets.action_space.n]) # cria a q_table, que é uma matriz de zeros \n",
    "# com o número de linhas igual ao número de estados e o número de colunas igual ao número de ações possíveis\n",
    "\n",
    "learning_rate = 0.1 # indica quanto o agente aprende a cada iteração\n",
    "discount_factor = 0.6 # fator de desconto, indicando o quanto o agente desconta o valor futuro em relação ao valor presente\n",
    "exploration = 0.1 # chance de escolher uma ação aleatória ao invés da melhor ação\n",
    "epochs = 10000 # número de iterações\n",
    "\n",
    "for taxi_run in range(epochs):\n",
    "    state = streets.reset()[0] # reseta o ambiente, para que o taxi comece em um estado aleatório\n",
    "    done = False\n",
    "    \n",
    "    while not done:\n",
    "        random_value = random.uniform(0, 1) # gera um valor aleatório entre 0 e 1, \n",
    "        # para decidir se o taxi vai explorar ou usar a melhor ação\n",
    "        if (random_value < exploration): # condição para explorar\n",
    "            action = streets.action_space.sample()  \n",
    "        else: # escolhe a melhor ação\n",
    "            action = np.argmax(q_table[state])\n",
    "        \n",
    "        next_state, reward, done, info, _ = streets.step(action)\n",
    "        # a função step executa a ação escolhida e retorna o próximo estado.\n",
    "        # leva em conta o nosso dicionário de recompensas.\n",
    "        # (a recompensa (valor de q), se ja foi feita, e se o passageiro foi deixado no destino)\n",
    "\n",
    "        prev_q = q_table[state, action] # valor de q do estado anterior\n",
    "        next_max_q = np.max(q_table[next_state]) # valor de q do próximo estado\n",
    "        new_q = (1 - learning_rate) * prev_q + learning_rate * (reward + discount_factor * next_max_q) \n",
    "        # calcula o novo valor de q para o estado atual \n",
    "        q_table[state, action] = new_q # atualiza o valor de q na q_table\n",
    "\n",
    "        state = next_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So now we have a table of Q-values that can be quickly used to determine the optimal next step for any given state! Let's check the table for our initial state above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-2.39850788, -2.40512677, -2.40460756, -2.3639511 , -7.64640915,\n",
       "       -8.39600262])"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_table[initial_state]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The lowest q-value here corresponds to the action \"go West\", which makes sense - that's the most direct route toward our destination from that point. It seems to work! Let's see it in action!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trip number 4 Step 8\n",
      "\n",
      "+---------+\n",
      "|\u001b[35mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : :\u001b[42m_\u001b[0m: : |\n",
      "| | : | : |\n",
      "|Y| : |B: |\n",
      "+---------+\n",
      "  (West)\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[80], line 18\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m# print(streets.render(mode='ansi')) # parece que há algum erro aqui, pois o ambiente não é renderizado\u001b[39;00m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m+\u001b[39mstreets\u001b[38;5;241m.\u001b[39mrender())\n\u001b[1;32m---> 18\u001b[0m \u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m.5\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     19\u001b[0m state \u001b[38;5;241m=\u001b[39m next_state\n\u001b[0;32m     20\u001b[0m trip_length \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from IPython.display import clear_output\n",
    "from time import sleep\n",
    "\n",
    "# permite visualizar o taxi se movendo no ambiente\n",
    "for tripnum in range(1, 11):\n",
    "    state = streets.reset()[0] # adicionei o [0] para que o código funcione\n",
    "   \n",
    "    done = False\n",
    "    trip_length = 0\n",
    "    \n",
    "    while not done and trip_length < 25:\n",
    "        action = np.argmax(q_table[state])\n",
    "        next_state, reward, done, info, _ = streets.step(action)\n",
    "        clear_output(wait=True)\n",
    "        print(\"Trip number \" + str(tripnum) + \" Step \" + str(trip_length))\n",
    "        # print(streets.render(mode='ansi')) # quando fui rodar o código, há um erro dizendo que\n",
    "        # o método render não aceita o argumento mode='ansi'\n",
    "        print(\"\\n\"+streets.render())\n",
    "        sleep(.5)\n",
    "        state = next_state\n",
    "        trip_length += 1\n",
    "        \n",
    "    sleep(2)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
